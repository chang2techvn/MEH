import type { SafetyRule, FlaggedContent, BannedTerm, SafetyMetric } from "./types"

export const mockSafetyRules: SafetyRule[] = [
  {
    id: "rule-1",
    name: "Harmful Content Filter",
    description: "Detects and blocks content that could be harmful to users",
    severity: "high",
    enabled: true,
    threshold: 75,
    category: "harmful",
    createdAt: "2023-11-15T10:30:00Z",
    lastUpdated: "2024-04-10T14:22:00Z",
  },
  {
    id: "rule-2",
    name: "Personal Information Detection",
    description: "Identifies and redacts personal information in user content",
    severity: "critical",
    enabled: true,
    threshold: 85,
    category: "personal",
    createdAt: "2023-12-05T09:15:00Z",
    lastUpdated: "2024-04-12T11:45:00Z",
  },
  {
    id: "rule-3",
    name: "Inappropriate Language Filter",
    description: "Filters out profanity and inappropriate language",
    severity: "medium",
    enabled: true,
    threshold: 65,
    category: "content",
    createdAt: "2024-01-20T16:40:00Z",
    lastUpdated: "2024-04-05T09:30:00Z",
  },
  {
    id: "rule-4",
    name: "Spam Detection",
    description: "Identifies and filters spam content",
    severity: "low",
    enabled: false,
    threshold: 60,
    category: "behavior",
    createdAt: "2024-02-10T13:20:00Z",
    lastUpdated: "2024-03-28T15:10:00Z",
  },
  {
    id: "rule-5",
    name: "Toxic Behavior Detection",
    description: "Identifies toxic behavior patterns in user interactions",
    severity: "high",
    enabled: true,
    threshold: 70,
    category: "behavior",
    createdAt: "2024-01-05T11:30:00Z",
    lastUpdated: "2024-04-15T10:20:00Z",
  },
]

export const mockFlaggedContent: FlaggedContent[] = [
  {
    id: "flag-1",
    content: "This content was flagged for potentially harmful language...",
    userId: "user-123",
    userName: "JohnDoe",
    timestamp: "2024-04-18T09:45:00Z",
    rule: "Harmful Content Filter",
    severity: "high",
    status: "pending",
    score: 82,
  },
  {
    id: "flag-2",
    content: "This submission contained personal information that was automatically redacted...",
    userId: "user-456",
    userName: "JaneSmith",
    timestamp: "2024-04-17T14:30:00Z",
    rule: "Personal Information Detection",
    severity: "critical",
    status: "reviewed",
    score: 91,
  },
  {
    id: "flag-3",
    content: "This message contained inappropriate language that was filtered...",
    userId: "user-789",
    userName: "MikeJohnson",
    timestamp: "2024-04-16T11:20:00Z",
    rule: "Inappropriate Language Filter",
    severity: "medium",
    status: "approved",
    score: 68,
  },
  {
    id: "flag-4",
    content: "This content was identified as potential spam...",
    userId: "user-101",
    userName: "SarahWilliams",
    timestamp: "2024-04-15T16:50:00Z",
    rule: "Spam Detection",
    severity: "low",
    status: "rejected",
    score: 62,
  },
  {
    id: "flag-5",
    content: "This interaction was flagged for toxic behavior patterns...",
    userId: "user-202",
    userName: "DavidBrown",
    timestamp: "2024-04-14T10:15:00Z",
    rule: "Toxic Behavior Detection",
    severity: "high",
    status: "pending",
    score: 76,
  },
]

export const mockBannedTerms: BannedTerm[] = [
  {
    id: "term-1",
    term: "badword1",
    category: "Profanity",
    replacement: "****",
    addedBy: "Admin",
    addedAt: "2024-03-10T09:30:00Z",
  },
  {
    id: "term-2",
    term: "badword2",
    category: "Hate Speech",
    replacement: "****",
    addedBy: "Moderator",
    addedAt: "2024-03-15T14:20:00Z",
  },
  {
    id: "term-3",
    term: "badphrase1",
    category: "Harassment",
    replacement: "******",
    addedBy: "Admin",
    addedAt: "2024-03-20T11:45:00Z",
  },
  {
    id: "term-4",
    term: "inappropriateterm1",
    category: "Inappropriate",
    replacement: "***************",
    addedBy: "System",
    addedAt: "2024-03-25T16:10:00Z",
  },
  {
    id: "term-5",
    term: "sensitiveinfo",
    category: "Personal Information",
    replacement: "[redacted]",
    addedBy: "Admin",
    addedAt: "2024-04-01T10:30:00Z",
  },
]

export const mockSafetyMetrics: SafetyMetric[] = [
  { name: "Content Safety Score", value: 92, change: 3, target: 95 },
  { name: "Flagged Content Rate", value: 2.4, change: -0.8, target: 2.0 },
  { name: "Response Time (min)", value: 4.2, change: -1.5, target: 3.0 },
  { name: "False Positive Rate", value: 3.1, change: -0.5, target: 2.5 },
]
